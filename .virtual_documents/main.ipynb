import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from itertools import combinations
from collections import Counter
import math
from functools import partial
from tqdm import tqdm
# from p_tqdm import p_map, p_umap, p_imap, p_uimap
from multiprocessing.pool import ThreadPool as Pool
from multiprocessing import Process
import time

plt.style.use('ggplot')


# This function implements the Knn classifier by computing the euclidean distance between data points
def knn(k, predicted_x, predicted_y, pool):
    
#     predicted_dp = np.array([predicted_x, predicted_y, 0.0])
#     distances = predicted_dp - pool
#     distances = np.power(distances, 2)
#     distances = distances[:,0:2] 
#     distances = np.sum(distances, axis=1)
#     distances = np.sqrt(distances)
    
#     vote_pool_indices = np.argsort(distances)[:k]
#     vote_pool_classes = [pool[i][2] for i in vote_pool_indices]
#     vote_result = Counter(vote_pool_classes).most_common()
    
#     return vote_result[0][0]

    predicted_dp = np.array([predicted_x, predicted_y])
    distances = predicted_dp - pool[:,:2]
    distances = np.power(distances, 2)
    # distances = distances[:,0:2] 
    distances = np.sum(distances, axis=1)
    # distances = np.sqrt(distances) # Unnecessary for correct computation

    vote_pool_indices = np.argsort(distances)[:k]
    vote_pool_classes = [int(pool[i][2]) for i in vote_pool_indices]

    vote_result = Counter(vote_pool_classes).most_common()
    return int(vote_result[0][0])


df = pd.read_csv('hw5data.txt', sep= ' ', names= ['x','y', 'class'])
df


df.plot(kind= 'hist', alpha=0.6)


df.plot(x= 'x', y = 'y', c='class', kind= 'scatter', colormap= 'Dark2', edgecolor='k')


# Create a dense 2D grid of data points 
mesh_X, mesh_Y = np.mgrid[0:1:15j,0:1:15j]

n = 20
K = 1

# Target classifier, g: It has a array P which is the classifier computed using all points in P (hw5data.txt). 
P = df.to_numpy() 
print(f'Dataset of target classifier: {P.shape}')

# Teaching/ student classifier, f: It has a array D which picks elements from P (hw5data.txt) D is a 
# subset or proper set of P, which holds the data points we want to include in teaching.
D = np.array([])
print(f'Dataset of teaching classifier: {D.shape}')


colors = {0: 'green', 1:'black'}
# View decision boundary of target classifier using all data points from the orignal pool
for X, Y in zip(mesh_X, mesh_Y):
    for x, y in zip(X, Y):
        result = knn(K, x,y, P)
        plt.scatter(x,y, c= colors[result],alpha= 0.5, marker='x')
        
temp_P = P.T
plt.scatter(temp_P[0], temp_P[1], c=temp_P[2], edgecolor='k', cmap='Dark2', s=20)
plt.title('Target classifier')
plt.xlabel('x')
plt.ylabel('y')
        
plt.show()


get_ipython().run_cell_magic("time", "", """# We precompute and store target classifier to avoid recomputation
target_class_mesh = []
# for i in range(len(mesh_X)):
#     res = []
#     for j in range(len(mesh_X[0])):
#         res.append(knn(K, mesh_X[i][j], mesh_Y[i][j], P))
        
#     target_class_mesh.append(res)

for dp in P[:, :2]:
    res = knn(K, dp[0], dp[1], P)
    target_class_mesh.append(res)
    
target_class_mesh = np.array(target_class_mesh)
print(f'Length of target mesh: {len(target_class_mesh)}')

print(f'target mesh:\n {target_class_mesh} ') # The top left corner of the mesh is at origin, hence explaining the 90 degree anti-clockwise turn in the plot.

#print(P[:,2:].flatten())""")


get_ipython().run_cell_magic("time", "", """# Enumeration
# This function finds all combinations of size n given a pool of data points
def find_combination(n, pool):
    datasets = combinations(pool, n)
    return np.array(list(datasets))""")


# Check if utilities class is imported successfully
import sys
import utilities 

modulename = 'utilities'
if modulename not in sys.modules:
    print('import not successful!')


import multiprocessing
# Check number of cpu cores
print(f'Max number of cores in this machine: {multiprocessing.cpu_count()}')


max_N = 20
util_obj = utilities.Utilities(max_N, K, mesh_X, mesh_Y, P, target_class_mesh)

def enumeration(n_choosen):
    start = time.time()
    possible_pool_Ds = find_combination(n_choosen, P)
    best_teaching_set = None
    best_teaching_cost = math.inf 
    searched_teaching_set_size = len(possible_pool_Ds)
    total_time = 0

    with Pool(processes = multiprocessing.cpu_count() - 7) as p:
        size = len(possible_pool_Ds)

        with tqdm(total= size) as pbar:
            for result in p.imap_unordered(util_obj.disagreement_func_ex, possible_pool_Ds):
                pbar.update()
                if best_teaching_cost > result[0]:
                    best_teaching_cost = result[0]
                    best_teaching_set = result[1]
                    
        p.close()
        p.join()
    end = time.time()
    total_time = end - start
    
    return best_teaching_set, best_teaching_cost, searched_teaching_set_size, total_time


def analyze_enumeration(n_start, n_stop):
     
    list_best_teaching_set = [] 
    list_best_teaching_cost = [] 
    list_number_of_teaching_sets = [] 
    list_total_time = [] 
    for n in range(n_start, n_stop+1):
        best_teaching_set, best_teaching_cost, number_of_teaching_sets, total_time = enumeration(n)
        list_best_teaching_set.append(best_teaching_set)
        list_best_teaching_cost.append(best_teaching_cost)
        list_number_of_teaching_sets.append(number_of_teaching_sets)
        list_total_time.append(total_time)
        
    return list_best_teaching_set, list_best_teaching_cost, list_number_of_teaching_sets, list_total_time


def print_analyze_enumeration(n_start, n_stop, style, teaching_colors):
    list_best_teaching_sets, list_best_teaching_cost, list_number_of_teaching_sets, list_total_time = analyze_enumeration(n_start, n_stop)
    
    ts_size = 2
    for best_teaching_set in list_best_teaching_sets:
        fig, ax = plt.subplots()
        ax.set_title(f'Enumeration: Teaching set data points $n$ = {ts_size}')
        ax.scatter(temp_P[0], temp_P[1], c=temp_P[2], cmap=style, s=20, alpha= 0.7)
        ts = best_teaching_set.T
        # plt.scatter(ts[0], ts[1], c= [teaching_colors[color] for color in ts[2]], edgecolor='k')
        size_order = 20 
        for dp in best_teaching_set:
            ax.scatter(dp[0], dp[1], c= teaching_colors[int(dp[2])], edgecolor='k', label= dp[2], s= size_order)
            size_order+= 5
           
        ax.legend(bbox_to_anchor = (1, 1), title= "Increasing size with each addition of teaching items")
        plt.show()
        ts_size += 1
    
    plt.title('Enumeration (N distinct runs): New teaching set size (random restarts) vs disagreement function cost')
    plt.xlabel('New teaching set size')
    plt.ylabel('Disagreement function cost')
    plt.plot(range(2, len(list_best_teaching_cost)+2), list_best_teaching_cost)
    plt.scatter(range(2, len(list_best_teaching_cost)+2), list_best_teaching_cost)
    plt.show()
    
    plt.title('Enumeration (N distinct runs): New teaching set size (random restarts) vs searched teaching set size')
    plt.xlabel('New teaching set size')
    plt.ylabel('Searched teaching set size')
    plt.plot(range(2, len(list_number_of_teaching_sets)+2), list_number_of_teaching_sets)
    plt.scatter(range(2, len(list_number_of_teaching_sets)+2), list_number_of_teaching_sets)
    plt.show()
    
    plt.title('Enumeration (N distinct runs): New teaching set size (random restarts) vs time elapsed (s)')
    plt.xlabel('New teaching set size')
    plt.ylabel('Time elapsed (s)')
    plt.plot(range(2, len(list_total_time)+2), list_total_time)
    plt.scatter(range(2, len(list_total_time)+2), list_total_time)
    plt.show()



teaching_colors = {0: 'yellow', 1:'red'}
# print_analyze_enumeration(2,2, 'Dark2', teaching_colors)


import random
# random.seed(10)


# Greedy
def greedy(n_stop):
    max_N = n_stop # Size of desired teaching set
    util_obj = utilities.Utilities(max_N, K, mesh_X, mesh_Y, P, P[:,2:])
    
    start = time.time()
    cost = 0
    best_teaching_cost = math.inf 
    curr_best_teaching_cost = math.inf 
    curr_best_teaching_set = np.array([])
    best_teaching_set = np.array([])
    searched_teaching_set_size = 0
    
    copy_P = np.copy(P)
    # Randomly pick a data point to make the teaching set's size = 1.
    idx = random.randint(0, len(copy_P)-1) 
    # Iterate over each point in the pool, and store the data point that yields the smallest disagreement function value into the teaching set   
   
    curr_D = np.array([copy_P[idx]])
    while not np.isinf(cost):
        curr_temp_best_teaching_cost = math.inf
        removed_idx = 0
        for i in range(len(copy_P)):
            searched_teaching_set_size += 1
            temp_D = np.append(curr_D, np.array([copy_P[i]]), axis=0)
            data = util_obj.disagreement_func_ex(temp_D)
            cost = data[0]
            if np.isinf(cost):
                break
            
            if cost < curr_temp_best_teaching_cost:
                curr_best_teaching_cost = cost
                curr_temp_best_teaching_cost = cost
                curr_best_teaching_set = temp_D 
                removed_idx = i
                
            if cost < best_teaching_cost:
                best_teaching_cost = cost
                best_teaching_set = temp_D 
       
        if not np.isinf(cost):
            curr_D = curr_best_teaching_set
            copy_P = np.delete(copy_P, removed_idx, axis=0)
        # print(curr_best_teaching_cost, len(curr_best_teaching_set))
   
    end = time.time()
    total_time = end - start 
    
    return curr_best_teaching_set, curr_best_teaching_cost, searched_teaching_set_size, total_time


def analyze_greedy(n_stop):
    list_best_teaching_set = [] 
    list_best_teaching_cost = [] 
    list_number_of_teaching_sets = [] 
    list_total_time = [] 
    for n in tqdm(range(2, n_stop+1)):
        best_teaching_set, best_teaching_cost, number_of_teaching_sets, total_time = greedy(n) 
        list_best_teaching_set.append(best_teaching_set)
        list_best_teaching_cost.append(best_teaching_cost)
        list_number_of_teaching_sets.append(number_of_teaching_sets)
        list_total_time.append(total_time)
        
    return list_best_teaching_set, list_best_teaching_cost, list_number_of_teaching_sets, list_total_time
    


def print_analyze_greedy(n_stop, style, teaching_colors):
    
    list_best_teaching_sets, list_best_teaching_cost, list_number_of_teaching_sets, list_total_time = analyze_greedy(n_stop)
  
    ts_size = 2
    for best_teaching_set in list_best_teaching_sets:
        fig, ax = plt.subplots()
        ax.set_title(f'Greedy: Teaching set data points $n$ = {ts_size} (random restarts)')
        ax.scatter(temp_P[0], temp_P[1], c=temp_P[2], cmap=style, s=20, alpha= 0.7)
        ts = best_teaching_set.T
        # plt.scatter(ts[0], ts[1], c= [teaching_colors[color] for color in ts[2]], edgecolor='k')
        size_order = 20 
        for dp in best_teaching_set:
            ax.scatter(dp[0], dp[1], c= teaching_colors[int(dp[2])], edgecolor='k', label= dp[2], s= size_order)
            size_order+= 5
           
        ax.legend(bbox_to_anchor = (1, 1), title= "Increasing size with each addition of teaching items")
        plt.show()
        ts_size += 1
    
    
    plt.title('Greedy (N distinct runs): New teaching set size (random restarts) vs disagreement function cost')
    plt.xlabel('New teaching set size')
    plt.ylabel('Disagreement function cost')
    plt.plot(range(2, len(list_best_teaching_cost)+2), list_best_teaching_cost)
    plt.scatter(range(2, len(list_best_teaching_cost)+2), list_best_teaching_cost)
    plt.show()
    
    plt.title('Greedy (N distinct runs): New teaching set size (random restarts) vs searched teaching set size')
    plt.xlabel('New teaching set size')
    plt.ylabel('Searched teaching set size')
    plt.plot(range(2, len(list_number_of_teaching_sets)+2), list_number_of_teaching_sets)
    plt.scatter(range(2, len(list_number_of_teaching_sets)+2), list_number_of_teaching_sets)
    plt.show()
    
    plt.title('Greedy (N distinct runs): New teaching set size (random restarts) vs time elapsed (s)')
    plt.xlabel('New teaching set size')
    plt.ylabel('Time elapsed (s)')
    plt.plot(range(2, len(list_total_time)+2), list_total_time)
    plt.scatter(range(2, len(list_total_time)+2), list_total_time)
    plt.show()


teaching_colors = {0: 'yellow', 1:'red'}
# print_analyze_greedy(20, 'Dark2', teaching_colors)


get_ipython().getoutput("pip show geopandas")


# Greedy
import geopandas as gpd
from geovoronoi import voronoi_regions_from_coords
from geovoronoi.plotting import subplot_for_map, plot_voronoi_polys_with_points_in_area


def greedy_state_visualizer(n_stop, teaching_color):
    max_N = n_stop # Size of desired teaching set
    util_obj = utilities.Utilities(max_N, K, mesh_X, mesh_Y, P, P[:,2:])
   
    
    # Picks an arbitrary start point
    idx = random.randint(0, len(util_obj.pool_P)-1) 
    
    local_best_teaching_dp_set = []
   
    local_best_teaching_dp_set.append(util_obj.pool_P[idx]) # Append the first datapoint for teaching set 
    local_best_teaching_dp_set = np.array(local_best_teaching_dp_set) # (1,2) array with one teaching point
    print(local_best_teaching_dp_set)
    
    # Calculate cost for the current teaching set
    cost, teaching_set, classifier_results = util_obj.disagreement_func_ex(local_best_teaching_dp_set)
    
    # Printing and plotting point
    fig, ax = plt.subplots(1,1)
    ax.set_title(f'Greedy: n = {len(local_best_teaching_dp_set)}, cost = {cost}')
    ax.set_box_aspect(1)
    ax.scatter(util_obj.pool_P.T[0], util_obj.pool_P.T[1], c =[teaching_color[c] for c in classifier_results])
    ax.scatter(local_best_teaching_dp_set.T[0],local_best_teaching_dp_set.T[1], marker='x')
    plt.show()
    
   
    # Declare variables
    temp_cost = cost # temporary cost 
    
    local_best_cost = np.inf # best cost
    local_best_next_teaching_dp_set = None
    local_best_classifier_results = None
    
    list_of_best_costs = np.array([cost])
    
    # Teaching point selection
    # Loops till our disagreement function returns infinity, signalling we have reach teaching set capacity
    print('Entering loop:::::::::::::::::')
    while not np.isinf(temp_cost):
        
        # Iterate all data points in pool, to select best next teaching point
        for data_point in util_obj.pool_P:
            temp_probe_teaching_set = np.append(local_best_teaching_dp_set, [data_point], axis = 0)
            
            # Find the cost, and classifier results after the addition of datapoint
            temp_cost, teaching_set, classifier_results = util_obj.disagreement_func_ex(temp_probe_teaching_set)
            
            if np.isinf(temp_cost):
                break
           
            # Store metadata for teaching set that generates the minimum cost 
            if temp_cost < local_best_cost:
                local_best_cost = temp_cost
                local_best_next_teaching_dp_set = temp_probe_teaching_set
                local_best_classifier_results = classifier_results
       
        if np.isinf(temp_cost):
            break
        
        # once for loop is left, we will have the best local minimums
        local_best_teaching_dp_set = local_best_next_teaching_dp_set
        list_of_best_costs = np.append(list_of_best_costs, [local_best_cost], axis = 0)
        
        
        # Draw voronoi
        world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))
        area = world[world.name == 'Italy']

        area = area.to_crs(epsg=3395)    # convert to World Mercator CRS
        area_shape = area.iloc[0].geometry   # get the Polygon
        
        region_polys, region_pts = voronoi_regions_from_coords(local_best_teaching_dp_set, area_shape)
        
        # Printing and plotting point
        fig, ax = plt.subplots(1,1)
        ax.set_title(f'Greedy: n = {len(local_best_teaching_dp_set)}, cost = {local_best_cost}')
        ax.set_box_aspect(1)
        ax.scatter(util_obj.pool_P.T[0], util_obj.pool_P.T[1], c =[teaching_color[c] for c in local_best_classifier_results])
        ax.scatter(local_best_teaching_dp_set.T[0],local_best_teaching_dp_set.T[1], marker='x')
        plt.show()
        
        # Once for loop is left, all local variables should be reset
        local_best_cost = np.inf
        
    
    return None


teaching_color = {0: 'indigo', 1: 'yellow'}
greedy_state_visualizer(20, teaching_color)


df2 = pd.read_csv('hw5bdata.txt', sep= ' ', names= ['x','y', 'class'])
df2


df2.plot(kind= 'hist', alpha=0.6)


df2.plot(x= 'x', y = 'y', c='class', kind= 'scatter', colormap= 'icefire', edgecolor='k')


# Create a dense 2D grid of data points 
mesh_X, mesh_Y = np.mgrid[-4:5:15j,-3:7:15j]

n = 20
K = 1

# Target classifier, g: It has a array P which is the classifier computed using all points in P (hw5data.txt). 
P = df2.to_numpy() 
print(f'Dataset of target classifier: {P.shape}')


## Target classifier's expected result with test data (2D mesh)
colors = {1: 'green', 2:'black', 3:'orange'}
# View decision boundary of target classifier using all data points from the orignal pool
for X, Y in zip(mesh_X, mesh_Y):
    for x, y in zip(X, Y):
        result = knn(K, x,y, P)
        plt.scatter(x,y, c= colors[result],alpha= 0.5, marker='x')
        
temp_P = P.T
plt.scatter(temp_P[0], temp_P[1], c=temp_P[2], edgecolor='k', cmap='icefire', s=20)
plt.title('Target classifier')
plt.xlabel('x')
plt.ylabel('y')
        
plt.show()


get_ipython().run_cell_magic("time", "", """# We precompute and store target classifier to avoid recomputation
target_class_mesh = []
# for i in range(len(mesh_X)):
#     res = []
#     for j in range(len(mesh_X[0])):
#         res.append(knn(K, mesh_X[i][j], mesh_Y[i][j], P))
        
#     target_class_mesh.append(res)

for dp in P[:, :2]:
    res = knn(K, dp[0], dp[1], P)
    target_class_mesh.append(res)
    
target_class_mesh = np.array(target_class_mesh)

print(f'target mesh:\n {target_class_mesh} ') # The top left corner of the mesh is at origin, hence explaining the 90 degree anti-clockwise turn in the plot.""")


max_N = 20
util_obj = utilities.Utilities(max_N, K, mesh_X, mesh_Y, P, target_class_mesh)


teaching_colors = {1: 'green', 2:'white', 3:'orange'}
# print_analyze_enumeration(2,2, 'icefire', teaching_colors)


teaching_colors = {1: 'green', 2:'white', 3:'orange'}
# print_analyze_greedy(20, 'icefire', teaching_colors)


# greedy_state_visualizer(20)



